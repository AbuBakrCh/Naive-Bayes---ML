{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6638a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess(stopWords, dirPath):    \n",
    "    processedReviews = []\n",
    "    for review in Path(dirPath).iterdir():\n",
    "        review = review.read_text().lower()\n",
    "        review = cleanStopWords(review)\n",
    "        processedReviews += [' '.join(re.findall(r'\\b([a-z]+)\\b', re.sub('[(),?!.;:\\n]|<br /><br />','', review)))]\n",
    "    return processedReviews\n",
    "        \n",
    "def cleanStopWords(review):\n",
    "    for stopword in stopWords.iloc[:,0]:\n",
    "        review = re.sub(rf'\\b{stopword}\\b', '', review)\n",
    "    return review\n",
    "\n",
    "def generateVocabulary(trainReviews):\n",
    "    global vocabulary\n",
    "    for review in trainReviews:\n",
    "        vocabulary.update(set(review.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0742bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "stopWordsDir = '/home/abu-bakr/Documents/ML@LUMS/Assignment 4/Dataset/stop_words.txt'\n",
    "stopWords = pd.read_csv(stopWordsDir, header=None)\n",
    "\n",
    "trainPosDir = '/home/abu-bakr/Documents/ML@LUMS/Assignment 4/Dataset/train/pos'\n",
    "trainPositiveReviews = preprocess(stopWords, trainPosDir)\n",
    "generateVocabulary(trainPositiveReviews)\n",
    "trainPositiveTokens = [word for review in trainPositiveReviews for word in review.split()]\n",
    "\n",
    "trainNegDir = '/home/abu-bakr/Documents/ML@LUMS/Assignment 4/Dataset/train/neg'\n",
    "trainNegativeReviews = preprocess(stopWords, trainNegDir)\n",
    "generateVocabulary(trainNegativeReviews)\n",
    "trainNegativeTokens = [word for review in trainNegativeReviews for word in review.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c9c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bdda9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcVocabularyLikelihood(wordCountDict):\n",
    "    global vocabulary\n",
    "    likelihood = dict()\n",
    "    laplaceDenominator = sum(wordCountDict.values()) + len(vocabulary)\n",
    "    for word in vocabulary:\n",
    "        likelihood[word] = np.log((wordCountDict[word] + 1)/laplaceDenominator)\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c3050ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "numDocuments = 25000\n",
    "numPositive = 12500\n",
    "numNegative = 12500\n",
    "\n",
    "logPrior = {'Positive': np.log(numPositive/numDocuments), 'Negative': np.log(numNegative/numDocuments)}\n",
    "\n",
    "\n",
    "posWordCountDict = Counter(trainPositiveTokens)\n",
    "negWordCountDict = Counter(trainNegativeTokens)\n",
    "\n",
    "\n",
    "\n",
    "logLikelihoodPosClass = calcVocabularyLikelihood(posWordCountDict)\n",
    "logLikelihoodNegClass = calcVocabularyLikelihood(negWordCountDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f0d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1af8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testClassification(testReviews, logLikelihoodPosClass, logLikelihoodNegClass, logPrior):\n",
    "    global vocabulary\n",
    "    predictions = {1: 0, -1: 0}\n",
    "    for review in testReviews:\n",
    "        posteriorPos = logPrior['Positive']\n",
    "        posteriorNeg = logPrior['Negative']\n",
    "        for word in review.split():\n",
    "            if word in vocabulary:\n",
    "                posteriorPos += logLikelihoodPosClass[word]\n",
    "                posteriorNeg += logLikelihoodNegClass[word]\n",
    "        if(posteriorPos >= posteriorNeg):\n",
    "            predictions[1] = predictions[1] + 1\n",
    "        else:\n",
    "            predictions[-1] = predictions[-1] + 1\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def calculateConfusionMatrix(predictedLabelsForPositiveClass, predictedLabelsForNegativeClass):\n",
    "    confusionMatrix = np.empty(shape=(2,2))\n",
    "    #for below dictionary for positive instances, key:1 contains the count of predicted positives\n",
    "    confusionMatrix[0][0] = predictedLabelsForPositiveClass[1]\n",
    "    #for below dictionary for negative instances, key:1 contains the count of predicted positives\n",
    "    confusionMatrix[0][1] = predictedLabelsForNegativeClass[1]\n",
    "    #for below dictionary for positive instances, key:-1 contains the count of predicted negatives\n",
    "    confusionMatrix[1][0] = predictedLabelsForPositiveClass[-1]\n",
    "    #for below dictionary for negative instances, key:-1 contains the count of predicted negatives\n",
    "    confusionMatrix[1][1] = predictedLabelsForNegativeClass[-1]\n",
    "    \n",
    "    confusionMatrix[np.isnan(confusionMatrix)] = 0\n",
    "    return confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c837a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPosDir = '/home/abu-bakr/Documents/ML@LUMS/Assignment 4/Dataset/test/pos'\n",
    "testPositiveReviews = preprocess(stopWords, testPosDir)\n",
    "\n",
    "testNegDir = '/home/abu-bakr/Documents/ML@LUMS/Assignment 4/Dataset/test/neg'\n",
    "testNegativeReviews = preprocess(stopWords, testNegDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3637f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[ 9606.  1484.]\n",
      " [ 2894. 11016.]]\n",
      "Accuracy: 0.82488\n"
     ]
    }
   ],
   "source": [
    "predictedLabelsForPositiveClass = testClassification(testPositiveReviews, logLikelihoodPosClass, logLikelihoodNegClass, logPrior)\n",
    "predictedLabelsForNegativeClass = testClassification(testNegativeReviews, logLikelihoodPosClass, logLikelihoodNegClass, logPrior)\n",
    "\n",
    "confusionMatrix = calculateConfusionMatrix(predictedLabelsForPositiveClass, predictedLabelsForNegativeClass)\n",
    "print(\"Confusion Matrix: \\n\", confusionMatrix)\n",
    "accuracy = (confusionMatrix[0][0] + confusionMatrix[1][1]) / np.sum(confusionMatrix)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "014bb8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82492\n",
      "Confusion Matrix: \n",
      " [[11024  1476]\n",
      " [ 2901  9599]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "trainAllReviews = trainPositiveReviews + trainNegativeReviews\n",
    "X_train = vectorizer.fit_transform(trainAllReviews)\n",
    "\n",
    "trainPositiveLabels = [1] * len(trainPositiveReviews)\n",
    "trainNegativeLabels = [-1] * len(trainNegativeReviews)\n",
    "y_train = trainPositiveLabels + trainNegativeLabels\n",
    "\n",
    "testAllReviews = testPositiveReviews + testNegativeReviews\n",
    "X_test = vectorizer.transform(testAllReviews)\n",
    "\n",
    "testPositiveLabels = [1] * len(testPositiveReviews)\n",
    "testNegativeLabels = [-1] * len(testNegativeReviews)\n",
    "y_test = testPositiveLabels + testNegativeLabels\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train,y_train)\n",
    "mnb_predicted = mnb.predict(X_test)\n",
    "\n",
    "#Please be lenient while penalizing. Complete assignment was submitted just an hour late. First draft\n",
    "#was on time. It had only slightly incomplete part 2.\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, mnb_predicted))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, mnb_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
